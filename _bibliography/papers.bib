---
---

@string{aps = {American Physical Society,}}

@book{einstein1920relativity,
  title={Relativity: the Special and General Theory},
  author={Einstein, Albert},
  year={1920},
  publisher={Methuen & Co Ltd},
  html={relativity.html}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{NeurIPS,
  abbr={NeurIPS},
  title={GOMAA-Geo: GOal Modality Agnostic Active Geo-localization},
  author={Sarkar, Anindya and Sastry, Srikumar and Pirinen, Aleksis and Zhang, Chongjie and Jacobs, Nathan and Vorobeychik, Yevgeniy},
  Abstract={We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities. This could emulate a UAV involved in a search-and-rescue operation navigating through an area, observing a stream of aerial images as it goes. The AGL task is associated with two important challenges. Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery). The second challenge is limited localization time (e.g., limited battery life, urgency) so that the goal must be localized as efficiently as possible, i.e. the agent must effectively leverage its sequentially observed aerial views when searching for the goal. To address these challenges, we propose GOMAA-Geo - a goal modality agnostic active geo-localization agent - for zero-shot generalization between different goal modalities. Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies. Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets - e.g., to disaster-hit areas without seeing a single disaster scenario during training - and goal modalities - e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views.},
  journal={In the 38th Neural Information Processing Systems, 2024},
  location={Vancouver},
  pages={1-23},
  numpages={23},
  year={2024},
  month={December},
  publisher={NeurIPS},
  pdf={https://arxiv.org/pdf/2406.01917v1},
  html={https://arxiv.org/abs/2406.01917v1},
  code={https://github.com/mvrl/gomaa-geo},
  dimensions={true},
  selected={true}
}

@article{AAMAS,
  abbr={AAMAS},
  title={Geospatial Active Search for Preventing Evictions},
  author={Sarkar, Anindya and Dichristofano, Alex and Das, Sanmay and Fowler, Patrick and Jacobs, Nathan and Vorobeychik, Yevgeniy},
  Abstract={Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hotspots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. We develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.},
  journal={In the 23rd International Conference on Autonomous Agents and Multiagent Systems, 2024},
  location={Auckland},
  pages={1-14},
  numpages={14},
  year={2024},
  month={May},
  publisher={AAMAS},
  pdf={https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2456.pdf},
  html={https://dl.acm.org/doi/abs/10.5555/3635637.3663192},
  code={https://github.com/anindyasarkarIITH/PSRL_VAS},
  dimensions={true},
  selected={true}
}

@article{NeurIPS,
  abbr={NeurIPS},
  title={A Partially-Supervised Reinforcement Learning Framework for Visual Active Search},
  author={Sarkar, Anindya and Jacobs, Nathan and Vorobeychik, Yevgeniy},
  Abstract={Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hotspots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. We develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.},
  journal={In the 37th Neural Information Processing Systems, 2023},
  location={New Orleans},
  pages={1-26},
  numpages={26},
  year={2023},
  month={December},
  publisher={NeurIPS},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2023/file/288b63aa98084366c4536ba0574a0f22-Paper-Conference.pdf},
  html={https://proceedings.neurips.cc/paper_files/paper/2023/hash/288b63aa98084366c4536ba0574a0f22-Abstract-Conference.html},
  code={https://github.com/anindyasarkarIITH/PSRL_VAS},
  dimensions={true},
  selected={true}
}

@article{NeurIPS,
  abbr={NeurIPS},
  title={How powerful are K-hop message passing graph neural networks},
  author={Feng, Jiarui and Chen, Yixin and Sarkar, Anindya and Li, Fuhai and Zhang, Muhan},
  Abstract={The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passingâ€”aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the WeisfeilerLehman (1-WL) test. Recently, researchers extended 1-hop message passing to K-hop message passing by aggregating information from K-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of K-hop message passing. In this work, we theoretically characterize the expressive power of K-hop message passing. Specifically, we first formally differentiate two different kernels of K-hop message passing which are often misused in previous works. We then characterize the expressive power of K-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that K-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves K-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.},
  journal={In the 36th Neural Information Processing Systems, 2022},
  location={New Orleans},
  pages={1-31},
  numpages={31},
  year={2022},
  month={December},
  publisher={NeurIPS},
  pdf={https://arxiv.org/pdf/2205.13328},
  html={https://dl.acm.org/doi/10.5555/3600270.3600615},
  code={https://github.com/JiaruiFeng/KP-GNN},
  dimensions={true},
  selected={true}
}

@article{CVPR,
  abbr={CVPR},
  title={A Framework for Learning Ante-hoc Explainable Models via Concepts},
  author={Sarkar, Anirban and Vijaykeerthy, Deepak and Sarkar, Anindya and Balasubramanian, Vineeth},
  Abstract={Self-explaining deep models are designed to learn the latent concept-based explanations implicitly during training, which eliminates the requirement of any post-hoc explanation generation technique. In this work, we propose one such model that appends an explanation generation module on top of any basic network and jointly trains the whole module that shows high predictive performance and generates meaningful explanations in terms of concepts. Our training strategy is suitable for unsupervised concept learning with much lesser parameter space requirements compared to baseline methods. Our proposed model also has provision for leveraging self-supervision on concepts to extract better explanations. However, with full concept supervision, we achieve the best predictive performance compared to recently proposed concept-based explainable models. We report both qualitative and quantitative results with our method, which shows better performance than recently proposed concept-based explainability methods. We reported exhaustive results with two datasets without ground truth concepts, i.e., CIFAR10, ImageNet, and two datasets with ground truth concepts, i.e., AwA2, CUB-200, to show the effectiveness of our method for both cases. To the best of our knowledge, we are the first ante-hoc explanation generation method to show results with a large-scale dataset such as ImageNet.},
  journal={In the IEEE / CVF Computer Vision and Pattern Recognition Conference, 2022},
  location={New Orleans},
  pages={1-16},
  numpages={16},
  year={2022},
  month={June},
  publisher={CVPR},
  pdf={https://arxiv.org/pdf/2108.11761v2},
  html={https://ieeexplore.ieee.org/document/9879843},
  code={https://github.com/anirbansarkar-cs/Ante-hoc_Explainability_Concepts},
  dimensions={true},
  selected={true}
}

@article{NeurIPS,
  abbr={NeurIPS},
  title={Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach},
  author={Sarkar, Anindya and Sarkar, Anirban and Gali, Sowrya and Balasubramanian, Vineeth},
  Abstract={Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.},
  journal={In the 35th Neural Information Processing Systems, 2021},
  location={Vancouver},
  pages={1-16},
  numpages={16},
  year={2021},
  month={December},
  publisher={NeurIPS},
  pdf={https://arxiv.org/pdf/2111.00295},
  html={https://dl.acm.org/doi/10.5555/3540261.3541244},
  code={https://github.com/sowgali/Get-Fooled-for-the-Right-Reason},
  dimensions={true},
  selected={true}
}

@article{AAAI,
  abbr={AAAI},
  title={Enhanced Regularizers for Attributional Robustness},
  author={Sarkar, Anindya and Sarkar, Anirban and Balasubramanian, Vineeth},
  Abstract={Deep neural networks are the default choice of learning models for computer vision tasks. Extensive work has been carried out in recent years on explaining deep models for vision tasks such as classification. However, recent work has shown that it is possible for these models to produce substantially different attribution maps even when two very similar images are given to the network, raising serious questions about trustworthiness. To address this issue, we propose a robust attribution training strategy to improve attributional robustness of deep neural networks. Our method carefully analyzes the requirements for attributional robustness and introduces two new regularizers that preserve a model's attribution map during attacks. Our method surpasses state-of-the-art attributional robustness methods by a margin of approximately 3% to 9% in terms of attribution robustness measures on several datasets including MNIST, FMNIST, Flower and GTSRB.},
  journal={In the 35th AAAI Conference on Artificial Intelligence, 2021},
  location={Vancouver},
  pages={1-16},
  numpages={16},
  year={2021},
  month={February},
  publisher={AAAI},
  pdf={https://arxiv.org/pdf/2012.14395},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/16355},
  code={https://github.com/anirbansarkar-cs/Enhanced_regularizers_attributional_robustness},
  dimensions={true},
  selected={true}
}



@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and SchrÃ¶dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
}
